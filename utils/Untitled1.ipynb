{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from abc import ABC\n",
    "from flask import Flask, request, jsonify\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if punkt tokenizer is available\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "# check if wordnet corpora is available\n",
    "try:\n",
    "    nltk.data.find(\"corpora/wordnet\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqChatbot():\n",
    "    intentss=None\n",
    "    vocabulary=None\n",
    "    classes=None\n",
    "    model=None\n",
    "    def __init__(self,name, intents_path, model_path,vocabulary_path,classes_path):\n",
    "        self.name = name\n",
    "        self.status = False\n",
    "        self.intentss = json.loads(open(intents_path).read())\n",
    "        self.vocabulary = pickle.load(open(vocabulary_path, \"rb\"))\n",
    "        self.classes = pickle.load(open(classes_path, \"rb\"))\n",
    "        self.model_name = model_path\n",
    "        # options for language detection models\n",
    "        self.lang_detect_model = [\"googletrans\", \"fasttext\"]\n",
    "        # initialize Lemmatizer instance\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def load(self):\n",
    "        self.model = load_model(self.model_name)\n",
    "        self.status = True\n",
    "        return True  \n",
    "\n",
    "\n",
    "\n",
    "    # preprocess user input text\n",
    "    def prepocess_text(self,text):\n",
    "    # tokenize the text\n",
    "        words = nltk.word_tokenize(text)\n",
    "    # lemmatize each word\n",
    "        words = [self.lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "        return words\n",
    "\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in vocab present in sentence\n",
    "    def bag_of_words(self,text, vocab, show_details=True):\n",
    "    # preprocess text\n",
    "        sentence_words = self.prepocess_text(text)\n",
    "    # bag of words - vocabulary matrix - matrix of N words\n",
    "        bag = [0] * len(vocab)\n",
    "        for s in sentence_words:\n",
    "            for i, w in enumerate(vocab):\n",
    "                if w == s:\n",
    "                # assign 1 if the word is in vocabulary\n",
    "                    bag[i] = 1\n",
    "                    if show_details:\n",
    "                        print(\"found in bag: %s\" % w)\n",
    "        return np.array(bag)\n",
    "\n",
    "\n",
    "# predict corresponding intent for input text\n",
    "    def predict_intent(self,sentence, model):\n",
    "        print(\"hello\")\n",
    "        p = self.bag_of_words(sentence, self.vocabulary, show_details=False)\n",
    "        # filter out predictions by threshold\n",
    "        res = model.predict(np.array([p]))[0]\n",
    "        ERROR_THRESHOLD = 0.25\n",
    "        results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "        # sort by probability score\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return_list = []\n",
    "        for r in results:\n",
    "            return_list.append({\"intent\": self.classes[r[0]], \"probability\": str(r[1])})\n",
    "        \n",
    "        return return_list\n",
    "\n",
    "\n",
    "# generate output response based on identified intent of input\n",
    "    def generate_response(self,ints, intents_json):\n",
    "        tag = ints[0][\"intent\"]\n",
    "        result=\"Sorry , we don't have this information , ask another question please .\"\n",
    "        list_of_intents = intents_json[\"intents\"]\n",
    "        for i in list_of_intents:\n",
    "            if i[\"tag\"] == tag:\n",
    "            # select a response at random from available responses\n",
    "                result = random.choice(i[\"responses\"])\n",
    "                break\n",
    "        return result\n",
    "\n",
    "\n",
    "# generate chatbot output for user input message\n",
    "    def chatbot_response(self,msg):\n",
    "        print(msg)\n",
    "        ints = self.predict_intent(msg, self.model)\n",
    "        res = self.generate_response(ints, self.intentss)\n",
    "        return res\n",
    "    def get_status(self):\n",
    "        return self.status\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gestion:\n",
    "    DataBase = 'Chatbots.json'\n",
    "    bots_objects = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        with open(self.DataBase, 'r') as myfile:\n",
    "            data=myfile.read()\n",
    "        bots = json.loads(data)[\"chatbots\"]\n",
    "        \n",
    "        \n",
    "        for bot in bots:\n",
    "            self.bots_objects[bot[\"intitule\"]] = Seq2SeqChatbot(bot[\"intitule\"],bot[\"json\"],bot[\"modelName\"],bot[\"vocabulary\"],bot[\"classes\"])\n",
    "           \n",
    "\n",
    "                \n",
    "    def load_bot(self, name):\n",
    "        if self.bots_objects[name].get_status():\n",
    "            return True\n",
    "        else:\n",
    "            return self.bots_objects[name].load()\n",
    "\n",
    "    def get_answer(self, name, query):\n",
    "        print(query)\n",
    "        if self.bots_objects[name].get_status():\n",
    "            return self.bots_objects[name].chatbot_response(query)\n",
    "        else:\n",
    "            return 'please wait'\n",
    "\n",
    "    #Creer une liste des chatbots pour l'utilisateur\n",
    "    def list(self):\n",
    "        lbot = []\n",
    "        for key in self.bots_objects:\n",
    "            lbot.append({\n",
    "                \"name\": key,\n",
    "                \"uri\": \"https://picsum.photos/200/300\",\n",
    "                \"color\": \"red\",\n",
    "                \"describtion\": \"Ur chatbot buddy\"\n",
    "            })\n",
    "        return lbot\n",
    "\n",
    "    def add_proposition(self, app, name, question, response):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creer l'instance du serveur flask\n",
    "import json\n",
    "app = Flask('__main__')\n",
    "\n",
    "#Creer l'instance du gestionnaire\n",
    "gst = Gestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/ask/<name>', methods=['GET'])\n",
    "def hello_world(name):\n",
    "    answer = gst.get_answer(name, str(request.args.get('query')))\n",
    "    return jsonify(ans=answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/load/<name>', methods=['GET'])\n",
    "def load_bot(name):\n",
    "    gst.load_bot(name)\n",
    "    return jsonify(ans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/list', methods=['Get'])\n",
    "def list_of_bots():\n",
    "    lbot = gst.list()\n",
    "    return jsonify(lbot)\n",
    "\n",
    "@app.route('/propose/<name>', methods=['Post'])\n",
    "def propose(name):\n",
    "    gst.add_proposition(app, name, request.args.get('question'), request.args.get('response'))\n",
    "    return jsonify(ans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://192.168.0.188:19000/ (Press CTRL+C to quit)\n",
      "192.168.0.188 - - [04/Jul/2021 18:42:20] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 18:42:30] \"\u001b[37mGET //list HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 18:42:30] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 18:45:26] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 18:45:53] \"\u001b[37mGET //list HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 18:45:53] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 18:45:54] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 18:45:56] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 18:46:01] \"\u001b[37mGET //load/where%20to%20go HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 18:46:28] \"\u001b[37mGET //load/where%20to%20go HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 18:46:32] \"\u001b[37mGET //load/hello HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 18:46:39] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 18:46:47] \"\u001b[37mGET //list HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 18:46:47] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 18:46:48] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 18:46:50] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 18:47:01] \"\u001b[37mGET //load/where%20to%20go HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:07:21] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:07:55] \"\u001b[37mGET //list HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:07:55] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:07:56] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:07:58] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:08:51] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:08:59] \"\u001b[37mGET //list HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:09:00] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:09:01] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:09:03] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:09:06] \"\u001b[37mGET //load/where%20to%20go HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:09:26] \"\u001b[37mGET //load/where%20to%20go HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:09:31] \"\u001b[37mGET //load/where%20to%20go HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:10:49] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:11:15] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:12:44] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:13:03] \"\u001b[37mGET //list HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:13:04] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:13:06] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:13:10] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:13:19] \"\u001b[37mGET //load/where%20to%20go HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:13:21] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:13:22] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:13:23] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:13:43] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:13:44] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:13:46] \"\u001b[37mGET //load/where%20to%20go HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FDB2DD8C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.0.188 - - [04/Jul/2021 19:13:54] \"\u001b[37mGET //ask/where%20to%20go?query=hello HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:17:18] \"\u001b[37mGET //load/where%20to%20go HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hhhh\n",
      "Hhhh\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-07-04 19:17:23,748] ERROR in app: Exception on /ask/where to go [GET]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\hp\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"<ipython-input-83-3bf55d06fd3e>\", line 3, in hello_world\n",
      "    answer = gst.get_answer(name, str(request.args.get('query')))\n",
      "  File \"<ipython-input-81-559486ca9f38>\", line 25, in get_answer\n",
      "    return self.bots_objects[name].chatbot_response(query)\n",
      "  File \"<ipython-input-80-cd5e03b20f84>\", line 83, in chatbot_response\n",
      "    res = self.generate_response(ints, self.intentss)\n",
      "  File \"<ipython-input-80-cd5e03b20f84>\", line 68, in generate_response\n",
      "    tag = ints[0][\"intent\"]\n",
      "IndexError: list index out of range\n",
      "192.168.0.188 - - [04/Jul/2021 19:17:23] \"\u001b[35m\u001b[1mGET //ask/where%20to%20go?query=Hhhh HTTP/1.1\u001b[0m\" 500 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:17:26] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:18:09] \"\u001b[37mGET //ask/where%20to%20go?query=hello HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.0.188 - - [04/Jul/2021 19:18:17] \"\u001b[37mGET //ask/where%20to%20go?query=yes HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.0.188 - - [04/Jul/2021 19:19:15] \"\u001b[37mGET //load/where%20to%20go HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whats up\n",
      "whats up\n",
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.0.188 - - [04/Jul/2021 19:19:23] \"\u001b[37mGET //ask/where%20to%20go?query=whats%20up HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:29:11] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:32:52] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:32:53] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:33:26] \"\u001b[37mGET //list HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:33:27] \"\u001b[37mGET //list HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:33:28] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:33:28] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:33:28] \"\u001b[37mGET //list HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:33:29] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:33:57] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:34:06] \"\u001b[37mGET //list HTTP/1.1\u001b[0m\" 200 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:34:06] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:34:07] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:34:11] \"\u001b[33mPOST /logs HTTP/1.1\u001b[0m\" 404 -\n",
      "192.168.0.188 - - [04/Jul/2021 19:37:43] \"\u001b[37mGET //load/hello HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "hello\n",
      "hello\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FDB2E95160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.168.0.188 - - [04/Jul/2021 19:37:51] \"\u001b[37mGET //ask/hello?query=hello HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(host='192.168.0.188',port='19000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
